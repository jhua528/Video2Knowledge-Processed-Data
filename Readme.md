# ğŸ“¦ Video2Knowledge â€” Experimental Results and Data
*Laboratory for Industry 4.0 Smart Manufacturing Systems (LISMS)*


This repository contains the **experimental results and supplementary data** referenced in the paper:

> Video2Knowledge: Extracting Temporally Consistent Task Knowledge from Monocular Video for Robot Skill Learning


---

## ğŸ“‚ Contents

This repository provides **processed results** under View-0 (left-front) derived from the experiments described in the paper, including:

- ğŸ¥ **Videos**  
  Demonstration, simulation, real cobot action and qualitative result visualizations.

- ğŸ–¼ï¸ **Images**  
  Pose renderings, and visualization outputs.

- ğŸ“ **Text Files**  
  Extracted structured task knowledge and evaluation summaries.

- ğŸ“„ **JSON Files**  
  Structured representations of motion_analysis, events, and robot action sequences.

These data are intended to support **result verification, qualitative analysis** of the reported findings.

---

## ğŸ“¬ Access to Full Data

For access to complete multi-view data, including intermediate outputs (e.g., object poses, hand poses) for review or research purposes, please contact the authors directly.

ğŸ“§ **Contact:**  
*The authorâ€™s email address*

jan.polzer@auckland.ac.nz
or 
x.xu@auckland.ac.nz
or
jinyi.huang@auckland.ac.nz

---

## ğŸ“„ License

This repository is provided for **academic use only**. Please refer to the paper for details.
